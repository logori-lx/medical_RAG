# File: server/indexer_template.py
# [æ­¤æ–‡ä»¶å°†è¢«å®¢æˆ·ç«¯ä¸‹è½½å¹¶é‡å‘½åä¸º git_guard_indexer.py]
import os
import shutil
import chromadb
from typing import List
from git import Repo # ä¾èµ– gitpython è‡ªåŠ¨å®šä½
from zhipuai import ZhipuAI
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import LanguageParser
from langchain_text_splitters import Language, RecursiveCharacterTextSplitter

# --- å…³é”®è·¯å¾„é…ç½® ---
try:
    # è‡ªåŠ¨å‘ä¸Šå¯»æ‰¾ .git æ‰€åœ¨çš„æ–‡ä»¶å¤¹ï¼ˆå³é¡¹ç›®æ ¹ç›®å½•ï¼‰
    repo_obj = Repo(".", search_parent_directories=True)
    REPO_PATH = repo_obj.working_tree_dir
except:
    REPO_PATH = "." # Fallback

# æ•°æ®åº“å­˜æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .git_guard æ–‡ä»¶å¤¹å†…ï¼ˆéšè—ä¸”éšé¡¹ç›®å­˜åœ¨ï¼‰
# è¿™æ ·æ¯ä¸ªé¡¹ç›®éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„ RAG åº“
DB_PATH = os.path.join(REPO_PATH, ".git_guard", "chroma_db")
API_KEY = os.getenv("MEDICAL_RAG") 

# --- ä¿æŒä¸å˜çš„é…ç½® ---
LANGUAGE_MAP = {
    ".py": (Language.PYTHON, "repo_python"),
    ".java": (Language.JAVA, "repo_java"),
    ".js": (Language.JS, "repo_js"),
    # ... å…¶ä»–è¯­è¨€ ...
}

# ... (ZhipuEmbeddingFunction ç±»ä¿æŒä¸å˜) ...
class ZhipuEmbeddingFunction(chromadb.EmbeddingFunction):
    def __init__(self, api_key):
        self.api_key = api_key
        self.client = ZhipuAI(api_key=api_key)
    def __call__(self, input: List[str]) -> List[List[float]]:
        response = self.client.embeddings.create(model="embedding-3", input=input)
        return [data.embedding for data in response.data]

def build_index():
    if not API_KEY:
        # å¦‚æœæ˜¯åå°é™é»˜è¿è¡Œï¼Œæ‰“å°æ—¥å¿—å³å¯
        print("âŒ API Key missing. Skipping indexing.")
        return

    print(f"ğŸš€ [Indexer] Scanning: {REPO_PATH}")
    print(f"ğŸ“‚ [Indexer] Database: {DB_PATH}")

    # æ³¨æ„ï¼šåœ¨æœ¬åœ°æ›´æ–°æ—¶ï¼Œé€šå¸¸æˆ‘ä»¬åšå¢é‡æ›´æ–°æ¯”è¾ƒå¤æ‚ã€‚
    # ä¸ºäº† MVP ç¨³å®šæ€§ï¼Œè¿™é‡Œä¾ç„¶é‡‡ç”¨"å…¨é‡è¦†ç›–"ç­–ç•¥ã€‚
    # ç”Ÿäº§ç¯å¢ƒå¯ä»¥ç”¨å¢é‡æ›´æ–° (Collection.upsert)
    if os.path.exists(DB_PATH):
        try:
            shutil.rmtree(DB_PATH)
        except: pass

    client = chromadb.PersistentClient(path=DB_PATH)
    emb_fn = ZhipuEmbeddingFunction(api_key=API_KEY)

    for suffix, (lang_enum, col_name) in LANGUAGE_MAP.items():
        # ... (è¿™é‡Œæ”¾å…¥ä¹‹å‰ä¿®å¤è¿‡çš„å¸¦ fallback çš„åŠ è½½é€»è¾‘) ...
        # ä¸ºäº†èŠ‚çœç¯‡å¹…ï¼Œæ ¸å¿ƒåŠ è½½é€»è¾‘åŒä¸Šä¸€æ¬¡å›ç­”çš„ä¿®å¤ç‰ˆ
        # è®°å¾—æŠŠ try-except parser é™çº§é€»è¾‘æ”¾è¿›å»
        
        # ç®€å†™ç¤ºä¾‹ï¼š
        loader = GenericLoader.from_filesystem(REPO_PATH, glob=f"**/*{suffix}")
        try:
            docs = loader.load()
        except:
            continue # Skip errors
            
        if not docs: continue
        
        splitter = RecursiveCharacterTextSplitter.from_language(
            language=lang_enum, chunk_size=1000, chunk_overlap=200
        )
        split_docs = splitter.split_documents(docs)
        
        col = client.get_or_create_collection(name=col_name, embedding_function=emb_fn)
        
        # ç®€å•çš„åˆ†æ‰¹å†™å…¥
        batch_ids = [f"{col_name}_{i}" for i in range(len(split_docs))]
        batch_texts = [d.page_content for d in split_docs]
        batch_metas = [{"source": d.metadata.get("source", "")} for d in split_docs]
        
        if batch_ids:
            col.add(ids=batch_ids, documents=batch_texts, metadatas=batch_metas)

    print("âœ… [Indexer] Local Knowledge Base Updated.")

if __name__ == "__main__":
    build_index()