# from __future__ import annotations
#
# """
# baseline_ragas_evaluation.py
#
# Function:
#   - Evaluate the answers generated by the "normal large model" (without RAG) using ragas + DeepSeek.
#   - Uses the same rewritten_query and reference as the RAG system (reused from ragas_evaluation.py).
#   - For easy comparison:
#         RAG system              --> ragas_eval_report.json
#         Regular large model     --> test1_ragas_eval_report_plain.json
#
# Front:
#     rag_generate_and_ragas_test. already run baseline_answer_generation.py
#        get generated_plain_answers.json
#     2. The project already contains ragas_evaluation.py, which includes:
#          - REFERENCE_LIST
#          - load_generated_answers
#          - build_ragas_input
#          - save_json
#          - run_ragas
#
# Dependency:
#     same as ragas_evaluation.py:
#     pip install "ragas>=0.3.0" "datasets>=2.0.0" \
#                 "langchain>=0.2.0" "langchain-community>=0.2.0"
#
# Environment variables:
#     DEEPSEEK_API_KEY  (Or hardcode in this file)
# """
#
# import os
# from typing import Any, Dict, List
#
# # Import common content from your existing ragas_evaluation.py file.
# from ragas_evaluation import (  # type: ignore
#     REFERENCE_LIST,
#     load_generated_answers,
#     build_ragas_input,
#     save_json,
#     run_ragas,
# )
#
# # ====== Configuration area ===============================================
#
# GENERATED_PLAIN_ANSWERS_PATH = "generated_plain_answers.json"
#
# RAGAS_INPUT_PLAIN_PATH = "ragas_eval_input_plain.json"
# RAGAS_REPORT_PLAIN_PATH = "test1_ragas_eval_report_plain.json"
#
# DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY") or "sk-3a59029618234b1496de29504891bf78"
#
# # ===============================================================
#
#
# def main() -> None:
#     print("=== Medical RAG: Baseline RAGAS Evaluation for General Large Model ===\n")
#
#     # rag_generate_and_ragas_test. read generated_plain_answers.json
#     try:
#         generated_plain: List[Dict[str, Any]] = load_generated_answers(
#             GENERATED_PLAIN_ANSWERS_PATH
#         )
#     except Exception as e:
#         print(f"[BASELINE-RAGAS] read {GENERATED_PLAIN_ANSWERS_PATH} fail：{e}")
#         return
#
#     print(
#         f"[BASELINE-RAGAS] successfully read generated_plain_answers.json，There are a total of {len(generated_plain)} samples."
#     )
#
#     # 2. Construct ragas input (aligned reference)
#     ragas_samples = build_ragas_input(generated_plain, REFERENCE_LIST)
#     save_json(ragas_samples, RAGAS_INPUT_PLAIN_PATH)
#     print(f"[BASELINE-RAGAS] The evaluation data has been written.: {RAGAS_INPUT_PLAIN_PATH}")
#
#     # 3. Evaluation using ragas + DeepSeek
#     try:
#         metrics = run_ragas(ragas_samples, deepseek_api_key=DEEPSEEK_API_KEY)
#     except ImportError as e:
#         print("[BASELINE-RAGAS] The dependency is not installed, so the evaluation cannot be performed.")
#         print("error message：", e)
#         return
#     except Exception as e:
#         print("[BASELINE-RAGAS] Error occurred while calling ragas for evaluation:", e)
#         print("If you only need to generate ragas_eval_input_plain.json for the time being, you can ignore this error.")
#         return
#
#     # 4. Print and save the evaluation results
#     if metrics:
#         print("\n[BASELINE-RAGAS] Evaluation results (average score for each indicator):")
#         for name, score in metrics.items():
#             try:
#                 print(f"  {name}: {float(score):.4f}")
#             except Exception:
#                 print(f"  {name}: {score}")
#         save_json(metrics, RAGAS_REPORT_PLAIN_PATH)
#         print(f"\n[BASELINE-RAGAS] The evaluation results have been written:{RAGAS_REPORT_PLAIN_PATH}")
#     else:
#         print("[BASELINE-RAGAS] The evaluation returned an empty result. Please check your ragas version or enter data.")
#
#
# if __name__ == "__main__":
#     main()

from __future__ import annotations

"""
baseline_ragas_evaluation.py

Function:
  rag_generate_and_ragas_test. read baseline_answer_generation.py generate generated_plain_answers.json
  2. read top5_most_similar_vectors.json (structure: query_id + topk_docs)
  3. Aligned by ID, generate ragas_eval_input_plain.json with the following structure:
        [
          {
            "user_input": "...",
            "response": "...",
            "retrieved_contexts": ["Document 1 text...", "Document 2 text...", ...],
            "reference": "Standard answer text..."
          },
          ...
        ]
  4. Optional: Call `ragas_evaluation.run_ragas` to run ragas, outputting `ragas_eval_report_plain.json`.
"""

import json
import os
from typing import Any, Dict, List

# Reuse the standard answer and ragas call function from your own ragas_evaluation.py file.
from ragas_evaluation import (  # type: ignore
    REFERENCE_LIST,
    run_ragas,
)

# ================== Configuration area ========================

# The answer file generated by baseline
GENERATED_PLAIN_ANSWERS_PATH = "generated_plain_answers.json"

# Your topk file (the one I just sent).
TOPK_VECTORS_PATH = "top5_most_similar_vectors.json"

# Output: A ragas input with four fields.
RAGAS_INPUT_PLAIN_PATH = "ragas_eval_input_plain.json"

# Output: Baseline ragas evaluation report
RAGAS_REPORT_PLAIN_PATH = "ragas_eval_report_plain.json"

# DeepSeek API Key (or simply the environment variable DEEPSEEK_API_KEY)
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY") or "sk-3a59029618234b1496de29504891bf78"

# ==================================================


def load_json_list(path: str) -> List[Any]:
    """Read a JSON file whose top-level structure is a list."""
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, list):
        raise ValueError(f"{path} The top-level JSON must be a list.")
    return data


def load_generated_plain_answers(path: str) -> List[Dict[str, Any]]:
    """
    The generated_plain_answers.json file follows these structural conventions:
    [
      {
        "id": rag_generate_and_ragas_test,
        "user_input": "...",
        "response": "...",
        "retrieved_contexts": []   # baseline It is empty when generated, and we do not depend on it here
      },
      ...
    ]
    """
    data = load_json_list(path)
    for idx, item in enumerate(data):
        if not isinstance(item, dict):
            raise ValueError(f"The {idx}th record in {path} is not an object (dict).")
        for k in ("id", "user_input", "response"):
            if k not in item:
                raise ValueError(f"The {path} record is missing the field {k}.")
    return data  # type: ignore[return-value]


def load_topk_contexts(path: str) -> Dict[int, List[str]]:
    """
    Precisely adapt the structure of your top5_most_similar_vectors.json file:

    [
      {
        "query_id": rag_generate_and_ragas_test,
        "topk_docs": [
          {"id": rag_generate_and_ragas_test, "metadata": {...}, "text": "Document 1..."},
          {"id": 2, "metadata": {...}, "text": "Document 2..."}
        ]
      },
      ...
    ]

    Return to dictionary:
        { query_id: [ "Document 1...", "Document 2...", ... ] }
    """
    data = load_json_list(path)
    id2ctx: Dict[int, List[str]] = {}

    for idx, item in enumerate(data):
        if not isinstance(item, dict):
            continue

        qid = item.get("query_id")
        if qid is None:
            # For security reasons, if there are other field names, you can add compatibility information here.
            # qid = item.get("id") or item.get("qid") ...
            continue

        docs = item.get("topk_docs") or []
        if not isinstance(docs, list):
            docs = [docs]

        ctx_texts: List[str] = []
        for d in docs:
            if isinstance(d, dict):
                # Prioritize text fields
                text = d.get("text")
                if text is None:
                    text = json.dumps(d, ensure_ascii=False)
                ctx_texts.append(str(text))
            else:
                ctx_texts.append(str(d))

        id2ctx[int(qid)] = ctx_texts

    return id2ctx


def build_reference_map() -> Dict[int, str]:
    """
    Map REFERENCE_LIST to {id: reference_text}.

    Convention:
    - REFERENCE_LIST[0] corresponds to id = rag_generate_and_ragas_test
    - REFERENCE_LIST[rag_generate_and_ragas_test] corresponds to id = 2
    - ...
    """
    ref_map: Dict[int, str] = {}
    for idx, item in enumerate(REFERENCE_LIST):
        ref_map[idx + 1] = item.get("reference", "")
    return ref_map


def build_ragas_input_plain(
    generated_plain: List[Dict[str, Any]],
    id2ctx: Dict[int, List[str]],
    ref_map: Dict[int, str],
) -> List[Dict[str, Any]]:
    """
    Integrating baseline generation results, TopK context, and references,

    the required structure for outputting `ragas_eval_input_plain.json` is as follows:

    [
      {
        "user_input": "...",
        "response": "...",
        "retrieved_contexts": ["Document 1...", "Document 2...", ...],
        "reference": "Standard Answer..."
      },
      ...
    ]
    """
    samples: List[Dict[str, Any]] = []

    for item in generated_plain:
        qid_raw = item.get("id")
        if qid_raw is None:
            continue

        try:
            qid = int(qid_raw)
        except Exception:
            continue

        user_input = item.get("user_input", "")
        response = item.get("response", "")

        ctx_texts = id2ctx.get(qid, [])
        reference = ref_map.get(qid, "")

        samples.append(
            {
                "user_input": user_input,
                "response": response,
                "retrieved_contexts": ctx_texts,
                "reference": reference,
            }
        )

    return samples


def save_json(data: Any, path: str) -> None:
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def main() -> None:
    print("=== Medical RAG: Baseline RAGAS Input Construction (Four-parameter version, adapted to topk_docs)===\n")

    # rag_generate_and_ragas_test. Read the baseline large model answer
    try:
        generated_plain = load_generated_plain_answers(GENERATED_PLAIN_ANSWERS_PATH)
    except Exception as e:
        print(f"[BASELINE-RAGAS] read {GENERATED_PLAIN_ANSWERS_PATH} fail：{e}")
        return
    print(
        f"[BASELINE-RAGAS] successfully read generated_plain_answers.json，total {len(generated_plain)} samples"
    )

    # 2. read TopK context
    try:
        id2ctx = load_topk_contexts(TOPK_VECTORS_PATH)
    except Exception as e:
        print(f"[BASELINE-RAGAS] read {TOPK_VECTORS_PATH} fail：{e}")
        return
    print(
        f"[BASELINE-RAGAS] successfully read top5_most_similar_vectors.json，There are a total of {len(id2ctx)} query_ids with context."
    )

    # 3. Construct an id -> reference mapping
    ref_map = build_reference_map()

    # 4. Combined into ragas_eval_input_plain.json
    ragas_samples = build_ragas_input_plain(generated_plain, id2ctx, ref_map)
    if not ragas_samples:
        print("[BASELINE-RAGAS] ragas_samples is null，please check if the id matches.")
        return

    save_json(ragas_samples, RAGAS_INPUT_PLAIN_PATH)
    print(
        f"[BASELINE-RAGAS] already generate {RAGAS_INPUT_PLAIN_PATH}，共 {len(ragas_samples)} 条样本。"
    )

    # Quick check: Print the first record of retrieved_contexts count to confirm it is not empty.
    first = ragas_samples[0]
    print(
        f"[BASELINE-RAGAS] Example: The number of retrieved_contexts for the rag_generate_and_ragas_test sample ="
        f"{len(first.get('retrieved_contexts', []))}"
    )

    # 5. Optional: Call ragas for evaluation (you can comment out the following section if not needed)
    try:
        metrics = run_ragas(ragas_samples, deepseek_api_key=DEEPSEEK_API_KEY)
    except Exception as e:
        print("[BASELINE-RAGAS] Call to ragas failed to evaluate. This error can be ignored; just use the generated JSON.")
        print("Error message:", e)
        return

    if metrics:
        print("\n[BASELINE-RAGAS] Evaluation results (average score for each indicator):")
        for name, score in metrics.items():
            try:
                print(f"  {name}: {float(score):.4f}")
            except Exception:
                print(f"  {name}: {score}")
        save_json(metrics, RAGAS_REPORT_PLAIN_PATH)
        print(f"\n[BASELINE-RAGAS]  evaluation results have been written: {RAGAS_REPORT_PLAIN_PATH}")
    else:
        print("The [BASELINE-RAGAS] evaluation result is empty. Please check your ragas version or enter data.")


if __name__ == "__main__":
    main()
